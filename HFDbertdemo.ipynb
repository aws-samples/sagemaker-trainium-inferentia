{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e677d93",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates how to use SageMaker with [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/ \"Trainium\") to train a text classification model, and then deploy the trained model in [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/ \"Inferentia\"). We are going to start with a [pretrained distilBERT model from Hugging Face](https://huggingface.co/distilbert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France.#distilbert-base-model-uncased \"distilbert\"), and fine-tune it with IMDB dataset. The training job will take place on `ml.trn1` instance which hosts the AWS Trainium accelerator. Then the trained model will be deployed in an endpoint in `ml.inf1` instance which hosts the AWS Inferentia accelerator.\n",
    "\n",
    "For SageMaker Studio environment during launch, we recommend using `Data Science` image, `Python 3` kernel, and use instance `ml.t3.medium`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7130f4a5",
   "metadata": {},
   "source": [
    "# Train a text classification model\n",
    "\n",
    "In this lab, you will use SageMaker to prepare and process the training data, and then execute a training job on AWS Trainium. Let's start this with installing necessary libraries and import them into the SageMaker runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973f0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.16.2 datasets==2.5.2 torch==1.11.0 ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64851d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker==2.116.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c0e0dc",
   "metadata": {},
   "source": [
    "Currently latest SageMaker version is 2.116.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ddc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import transformers\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sagemaker import utils\n",
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "from pathlib import Path\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from datetime import datetime\n",
    "import json\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c04e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check SageMaker SDK version\n",
    "print(sagemaker.__version__) # expect 2.116.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd10460",
   "metadata": {},
   "source": [
    "## Create Sagemaker session\n",
    "\n",
    "Next, create a SageMaker session and define an execution role. Default role should suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1540a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3684f615",
   "metadata": {},
   "source": [
    "The default bucket name printed is where all the data (model artifact, training script, and model checlpoints are going to be saved. Below, we also define a few more parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866bed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_count = 1\n",
    "source_dir = 'scripts'\n",
    "\n",
    "bucket=sagemaker.Session().default_bucket()\n",
    "base_job_name=\"imdb-2022106-2xl\"\n",
    "checkpoint_in_bucket=\"checkpoints\" # dir name to hold pt file. \n",
    "\n",
    "# The S3 URI to store the checkpoints\n",
    "checkpoint_s3_bucket=\"s3://{}/{}/{}\".format(bucket, base_job_name, checkpoint_in_bucket)\n",
    "print(checkpoint_s3_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41882873",
   "metadata": {},
   "source": [
    "## Preprocessing and tokenization\n",
    "\n",
    "In this section, we are going to use Hugging Face API to download a small dataset for text classification. This is the dataset we will use to train a model for our text classification task. Once the dataset is downloaded, we will then persist this dataset in our default S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3bc6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'imdb'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'HFDatasets/imdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93489d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86dfd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect data\n",
    "dataset['train'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf76128",
   "metadata": {},
   "source": [
    "## Uploading data to `sagemaker_session_bucket`\n",
    "\n",
    "Here, we use a `S3FileSystem` interface to define our SageMaker session's connection to the default S3 bucket, this interface is used as an input to `save_to_disk` API, so dataset is stored in the designated S3 path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea77a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3FileSystem()  \n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}'\n",
    "dataset.save_to_disk(training_input_path,fs=s3) # uncomment to save or overwrite data in s3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c6cab",
   "metadata": {},
   "source": [
    "Now the training data is stored in S3, we are going to develop the training script and define a PyTorch Estimator to run the training script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed82ddb4",
   "metadata": {},
   "source": [
    "## Fine-tuning & start Sagemaker Training Job\n",
    "\n",
    "A training script is required for SageMaker PyTorch estimator to run a model training job. Below is the script for fine-tuning a pretrained Hugging Face distilBERT model with the dataset (IMDB movie review) we just put in the S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd97828",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./scripts/dbert_train_torchrun_trn1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cec8ee",
   "metadata": {},
   "source": [
    "In the training script, there are several important details worth mentioning:\n",
    "\n",
    "1. **distributed training (hardware)** This is an example of data parallel distributed training. In this training scenario, since there are multiple NeuronCores in this `trn1` instance, each NeuronCore receives a copy of the model and a shard of data. Each NeuronCore is managed by a worker that runs a copy of the training script. Gradient from each worker is aggregated and averaged, such that each worker receives exactly same updates to the model weights. Then another iteration of training resumes.  \n",
    "\n",
    "\n",
    "2. **Distributed training (software)** A specialized backend `torch.xla.distributed.xla_backend` is required for PyTorch to run on XLA device such as Trainium. In the training loop, since each worker generates its own gradient, `xm.optimiser_Step(optimizer)` makes sure all workers receive same gradient update before next iteration of training. \n",
    "\n",
    "3. **Bring your own training data** Hugging Face provides `load_from_disk` API to load training data specified by an S3 path. SageMaker uses an environment veriable `SM_CHANNEL_TRAIN` to track the S3 path back when we uploaded the training data in a previous cell. Thus `SM_CHANNEL_TRAIN` is used as an input to `load_from_disk` API.\n",
    "\n",
    "4. **Persist trained weights** Trained weights are stored in SageMaker session's default S3 bucket. The session's bucket is managed by SageMaker. You may leverage environment variable `SM_MODEL_DIR` to access this bucket and write the trained weights in this bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8518a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_count = 1\n",
    "num_cores = 2\n",
    "source_dir = 'scripts'\n",
    "\n",
    "bucket=sagemaker.Session().default_bucket()\n",
    "base_job_name=\"imdb-classification\"\n",
    "checkpoint_in_bucket=\"checkpoints\" # dir name to hold pt file. \n",
    "\n",
    "# The S3 URI to store the checkpoints\n",
    "checkpoint_s3_bucket=\"s3://{}/{}/{}\".format(bucket, base_job_name, checkpoint_in_bucket)\n",
    "print(checkpoint_s3_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_estimator = PyTorch(\n",
    "    entry_point=\"dbert_train_torchrun_trn1.py\", # Specify your train script\n",
    "    source_dir=\"scripts\",\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type='ml.trn1.2xlarge',\n",
    "    framework_version='1.11.0',\n",
    "    py_version='py38',\n",
    "    disable_profiler=True,\n",
    "    output_path=checkpoint_s3_bucket,\n",
    "    base_job_name=base_job_name,\n",
    "    \n",
    "    # Parameters required to enable checkpointing\n",
    "    checkpoint_s3_uri=checkpoint_s3_bucket,\n",
    "    volume_size = 512,\n",
    "    distribution={\n",
    "        \"torch_distributed\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "pt_estimator.fit({'train': training_input_path}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f64221",
   "metadata": {},
   "source": [
    "To find out S3 path where the trained weights are stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de863e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_uri = pt_estimator.model_data \n",
    "print(model_path_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fefc88",
   "metadata": {},
   "source": [
    "Now that model training work is done, and the model weights are stored to S3, from here and on, we are going to focus on inference. The trained model weight file compressed and stored in the S3 path shown above. It is a dictionary, which contains keys that matche the original Hugging Face distilbert model. This means the trained weight may to run on any hardware platform, as long as the Hugging Face library installed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd01832",
   "metadata": {},
   "source": [
    "# Lab 2: Inference using trained model\n",
    "\n",
    "These are the overall steps in this section of the notebook: 1. load weights to the model, 2. trace it with [`torch.jit.trace`](https://pytorch.org/docs/stable/generated/torch.jit.trace.html) API to convert the model into a torchscript, 3. compile torchscript for deployment in Inferentia, 4. deploy the model to an endpoint, 5. send test query to the endppoint via http request.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d179d6",
   "metadata": {},
   "source": [
    "## Load model weights\n",
    "\n",
    "Load the tokenizer and model from Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fede77",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", return_dict=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff0f758",
   "metadata": {},
   "source": [
    "Extract parts of `model_path_uri`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8847a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bucket = model_path_uri.replace(\"s3://\",\"\").split(\"/\")[0] # get bucket name.\n",
    "prefix_path = \"/\".join(model_path_uri.replace(\"s3://\",\"\").split(\"/\")[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5be9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.download_data(\"./\",model_bucket,prefix_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481b26cf",
   "metadata": {},
   "source": [
    "Now you should see a `model.tar.gz` in the current directory. This is the trained weights. Unzip it and put it in `distilbert` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff44a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r distilbert\n",
    "! mkdir distilbert\n",
    "! tar -xvf ./model.tar.gz -C ./distilbert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db66ab69",
   "metadata": {},
   "source": [
    "Now load the weights in `checkppint.pt` into the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e6b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./distilbert/checkpoint.pt\")['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687751e4",
   "metadata": {},
   "source": [
    "## Trace model with `torch.jit` and upload torchscript to S3\n",
    "\n",
    "The model now is loaded with trained weights. The model needs to be converted to a torchscript format first, before it can be compiled for Inferentia. Let's convert it and save it to `traced_model` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed66e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for model artifacts\n",
    "Path(\"traced_model/\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8fe691",
   "metadata": {},
   "source": [
    "To trace a model into a torchscript (`.pth`), it requires the model and an `example_input`, which is a tensor or tuple of tensors. `torch.jit.trace` is actually recording all the tensor ops during the forward pass. The result is a torchscript file that captures the sequence of ops applied to the input tensor and all the way through the output nodes. This is a model graph.\n",
    "\n",
    "Therefore, we will create a sample input to `jit.trace` our model with PyTorch; this is a required step to have SageMaker Neo compile our model artifact, which will take a `tar.gz` file containing the traced model.\n",
    "\n",
    "The `.pth` extension when saving our model is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9fa6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sample input for jit model tracing\n",
    "seq_0 = \"I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered controversial I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.\"\n",
    "seq_1 = seq_0\n",
    "max_length = 512\n",
    "\n",
    "tokenized_sequence_pair = tokenizer.encode_plus(\n",
    "    seq_0, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "example = tokenized_sequence_pair[\"input_ids\"], tokenized_sequence_pair[\"attention_mask\"]\n",
    "\n",
    "traced_model = torch.jit.trace(model.eval(), example)\n",
    "traced_model.save(\"traced_model/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a1481",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czvf traced_model.tar.gz -C traced_model . && mv traced_model.tar.gz traced_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681755ac",
   "metadata": {},
   "source": [
    "We upload the traced model tar.gz file to Amazon S3, where our compilation job will download it from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645fcee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model_url = sess.upload_data(\n",
    "    path=\"traced_model/traced_model.tar.gz\",\n",
    "    key_prefix=\"neuron-experiments/bert-seq-classification/traced-model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10d0845",
   "metadata": {},
   "source": [
    "## Understanding our inference code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d122fe3",
   "metadata": {},
   "source": [
    "Before we deploy any model, let's check out the code we have written to do inference on a SageMaker endpoint, with a default uncompiled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0191d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize code/inference_inf1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb8b742",
   "metadata": {},
   "source": [
    "As usual, we have a `model_fn` - receives the model directory, is responsible for loading and returning the model -, an `input_fn` and `output_fn` - in charge of pre-processing/checking content types of input and output to the endpoint - and a `predict_fn`, which receives the outputs of `model_fn` and `input_fn` (meaning, the loaded model and the deserialized/pre-processed input data) and defines how the model will run inference.\n",
    "\n",
    "\n",
    "#### Now, lets see what changes in the inference code when we want to do inference with a model that has been compiled for Inferentia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebe19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s model_fn code/inference_inf1.py\n",
    "def model_fn(model_dir):\n",
    "    \n",
    "    model_dir = '/opt/ml/model/'\n",
    "    dir_contents = os.listdir(model_dir)\n",
    "    model_path = next(filter(lambda item: 'model' in item, dir_contents), None)\n",
    "    \n",
    "    tokenizer_init = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    model = torch.jit.load(os.path.join(model_dir, model_path))\n",
    "\n",
    "    \n",
    "    return (model, tokenizer_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdf2055",
   "metadata": {},
   "source": [
    "In this case, within the `model_fn` we first grab the model artifact located in `model_dir` (the compilation step will name the artifact `model_neuron.pt`, but we just get the first file containing `model` in its name for script flexibility). Then, **we load the Neuron compiled model with `torch.jit.load`**. \n",
    "\n",
    "Other than this change to `model_fn`, we only need to add an extra import `import torch_neuron` to the beginning of the script, and get rid of all `.to(device)` calls, since the Neuron runtime will take care of loading our model to the NeuronCores on our Inferentia instance. All other functions are unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0243f7cc",
   "metadata": {},
   "source": [
    "## Compile model for Inferentia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03008b01",
   "metadata": {},
   "source": [
    "We now create a new `PyTorchModel` that will use `inference_inf1.py` as its entry point script. PyTorch version 1.5.1 is the latest that supports Neo compilation to Inferentia, as you can see from the warning in the compilation cell output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b560cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"neuron-experiments/bert-seq-classification\"\n",
    "flavour = \"normal\"\n",
    "date_string = datetime.now().strftime(\"%Y%m-%d%H-%M%S\")\n",
    "\n",
    "compiled_sm_model = PyTorchModel(\n",
    "    model_data=traced_model_url,\n",
    "    predictor_cls=Predictor,\n",
    "    framework_version=\"1.5.1\",\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    entry_point=\"inference_inf1.py\",\n",
    "    source_dir=\"code\",\n",
    "    py_version=\"py3\",\n",
    "    name=f\"{flavour}-distilbert-pt181-{date_string}\",\n",
    "    env={\"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"10\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2857503",
   "metadata": {},
   "source": [
    "Finally, we are ready to compile the model. Two notes here:\n",
    "* HuggingFace models should be compiled to `dtype` `int64`\n",
    "* the format for `compiler_options` differs from the standard Python `dict` that you can use when compiling for \"normal\" instance types; for inferentia, you must provide a JSON string with CLI arguments, which correspond to the ones supported by the [Neuron Compiler](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-cc/command-line-reference.html) (read more about `compiler_options` [here](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_OutputConfig.html#API_OutputConfig_Contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b23394",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hardware = \"inf1\"\n",
    "flavour = \"compiled-inf\"\n",
    "compilation_job_name = f\"distilbert-{flavour}-{hardware}-\" + date_string\n",
    "\n",
    "compiled_inf1_model = compiled_sm_model.compile(\n",
    "    target_instance_family=f\"ml_{hardware}\",\n",
    "    input_shape={\"input_ids\": [1, 512], \"attention_mask\": [1, 512]},\n",
    "    job_name=compilation_job_name,\n",
    "    role=role,\n",
    "    framework=\"pytorch\",\n",
    "    framework_version=\"1.5.1\",\n",
    "    output_path=f\"s3://{sess.default_bucket()}/{prefix}/neo-compilations/{flavour}-model\",\n",
    "    compiler_options=json.dumps(\"--dtype int64\"),\n",
    "    #     compiler_options={'dtype': 'int64'},    # For compiling to \"normal\" instance types, cpu or gpu-based\n",
    "    compile_max_run=900,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6a235e",
   "metadata": {},
   "source": [
    "You may expect to have output such as this:\n",
    "\n",
    "Expect similar results:\n",
    "\n",
    "?????????????????????????????......................................................................................!CPU times: user 408 ms, sys: 45.3 ms, \n",
    "\n",
    "total: 453 ms\n",
    "\n",
    "Wall time: 9min 49s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a305440a",
   "metadata": {},
   "source": [
    "After successful compilation, we deploy our model to an inf1.xlarge instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56671479",
   "metadata": {},
   "source": [
    "## Deploy compiled model to inf1 instance\n",
    "\n",
    "AWS Inf1 instance hosts the Inferentia accelerator. Now deploy the compiled model to this instance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bc0055",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "date_string = datetime.now().strftime(\"%Y%m-%d%H-%M%S\")\n",
    "\n",
    "compiled_inf1_predictor = compiled_inf1_model.deploy(\n",
    "    instance_type=\"ml.inf1.xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=f\"test-neo-{hardware}-{date_string}\",\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9477e2f4",
   "metadata": {},
   "source": [
    "Again, we test if everything is running smoothly in our endpoint. The result may vary, depending on model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c69d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with model endpoint\n",
    "payload1 = \"The new Hugging Face SageMaker DLC makes it super easy to deploy models in production. I love it!\"\n",
    "compiled_inf1_predictor.predict(payload1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8ae02f",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96e6b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_inf1_predictor.delete_model()\n",
    "compiled_inf1_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19d2ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
