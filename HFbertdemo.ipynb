{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df725833",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates how to use SageMaker with [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/ \"Trainium\") to train a text classification model, and then deploy the trained model in [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/ \"Inferentia\"). We are going to start with a [pretrained BERT model from Hugging Face](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France.#bert-base-model-uncased), and fine-tune it with IMDB dataset. This dataset consists of sentences labeled to be either positive or negative sentiment. The training job will take place on `ml.trn1` instance which hosts the AWS Trainium accelerator. Then the trained model weights will be deployed in an endpoint in `ml.inf1` instance which hosts the AWS Inferentia accelerator. You will send a sentence to the endpoint, and the model will predict the sentiment of the input sentence.\n",
    "\n",
    "For SageMaker Studio environment during launch, we recommend using Image `Pytorch 1.10 Python 3.8 CPU Optimized` with `Python 3` kernel, and use instance `ml.t3.medium`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a03d9d",
   "metadata": {},
   "source": [
    "# Train a text classification model\n",
    "\n",
    "In this notebook, you will use SageMaker to prepare and process the training data, and then execute a training job on AWS Trainium. Once the model is trained, you will deploy it to an endpoint. This endpoint will accept a sentence in plain text, and the model will classify this sentence as either positive or negative sentiment. Let's start this with installing necessary libraries and import them into the SageMaker runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b7031e-d537-4ef0-8502-e66550baaff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db731e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir datasets==2.11.0 torch==1.11.0 ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d9ea5b-1e79-4c6a-9c7e-cb8f242e7991",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall transformers==4.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104ac187",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker==2.116.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f489cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade s3fs botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e75767-a9ab-413c-a564-7bf70491edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 list | grep datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a081b6",
   "metadata": {},
   "source": [
    "Currently latest SageMaker version is 2.116.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34259f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import transformers\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sagemaker import utils\n",
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "from pathlib import Path\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from datetime import datetime\n",
    "import json\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check SageMaker SDK version\n",
    "print(sagemaker.__version__) # expect 2.116.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1add5ed9-5387-4193-b603-b0b0474c0949",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformers.__version__) # expect 4.17.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b520d2a5",
   "metadata": {},
   "source": [
    "## Create Sagemaker session\n",
    "\n",
    "Next, create a SageMaker session and define an execution role. Default role should suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff12f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ad926",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e1c2d",
   "metadata": {},
   "source": [
    "The default bucket name printed is where all the data (model artifact, training script, and model checlpoints are going to be saved. Below, we also define a few more parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb9f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_count = 1\n",
    "source_dir = 'train_scripts'\n",
    "\n",
    "bucket=sagemaker.Session().default_bucket()\n",
    "base_job_name=\"imdb-classification\"\n",
    "checkpoint_in_bucket=\"checkpoints\" # dir name to hold pt file. \n",
    "\n",
    "# The S3 URI to store the checkpoints\n",
    "checkpoint_s3_bucket=\"s3://{}/{}/{}\".format(bucket, base_job_name, checkpoint_in_bucket)\n",
    "print(checkpoint_s3_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1704e",
   "metadata": {},
   "source": [
    "## Preprocessing and tokenization\n",
    "\n",
    "In this section, we are going to use Hugging Face API to download a small dataset for text classification. This is the dataset we will use to train a model for our text classification task. Once the dataset is downloaded, we will then persist this dataset in our default S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfab1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = 'bert-base-uncased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'imdb'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'HFDatasets/imdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a30f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d796133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect data\n",
    "dataset['train'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac32f26",
   "metadata": {},
   "source": [
    "## Uploading data to `sagemaker_session_bucket`\n",
    "\n",
    "Here, we use a `S3FileSystem` interface to define our SageMaker session's connection to the default S3 bucket, this interface is used as an input to `save_to_disk` API, so dataset is stored in the designated S3 path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecdae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3FileSystem()  \n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}'\n",
    "dataset.save_to_disk(training_input_path,fs=s3) # uncomment to save or overwrite data in s3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f093922",
   "metadata": {},
   "source": [
    "Now the training data is stored in S3, we are going to develop the training script and define a PyTorch Estimator to run the training script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a81c57",
   "metadata": {},
   "source": [
    "## Fine-tuning & start Sagemaker Training Job\n",
    "\n",
    "A training script is required for SageMaker PyTorch estimator to run a model training job. Below is the script for fine-tuning a pretrained Hugging Face distilBERT model with the dataset (IMDB movie review) we just put in the S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a62f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./train_scripts/bert_train_torchrun_trn1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0868bf07",
   "metadata": {},
   "source": [
    "In the training script, there are several important details worth mentioning:\n",
    "\n",
    "1. **distributed training (hardware)** This is an example of data parallel distributed training. In this training scenario, since there are multiple NeuronCores in this `trn1` instance, each NeuronCore receives a copy of the model and a shard of data. Each NeuronCore is managed by a worker that runs a copy of the training script. Gradient from each worker is aggregated and averaged, such that each worker receives exactly same updates to the model weights. Then another iteration of training resumes.  \n",
    "\n",
    "\n",
    "2. **Distributed training (software)** A specialized backend `torch.xla.distributed.xla_backend` is required for PyTorch to run on XLA device such as Trainium. In the training loop, since each worker generates its own gradient, `xm.optimiser_Step(optimizer)` makes sure all workers receive same gradient update before next iteration of training. \n",
    "\n",
    "3. **Bring your own training data** Hugging Face provides `load_from_disk` API to load training data specified by an S3 path. SageMaker uses an environment veriable `SM_CHANNEL_TRAIN` to track the S3 path back when we uploaded the training data in a previous cell. Thus `SM_CHANNEL_TRAIN` is used as an input to `load_from_disk` API.\n",
    "\n",
    "4. **Persist trained weights** Trained weights can be used on different hardware targets. Weights are stored in SageMaker session's default S3 bucket. The session's bucket is managed by SageMaker. You may leverage environment variable `SM_MODEL_DIR` to access this bucket and write the trained weights in this bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_count = 1\n",
    "num_cores = 2\n",
    "source_dir = 'train_scripts'\n",
    "date_string = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "bucket=sagemaker.Session().default_bucket()\n",
    "base_job_name=\"imdb-classification-\" + str(date_string)\n",
    "checkpoint_in_bucket=\"checkpoints\" # dir name to hold pt file. \n",
    "\n",
    "# The S3 URI to store the checkpoints\n",
    "checkpoint_s3_bucket=\"s3://{}/{}/{}\".format(bucket, base_job_name, checkpoint_in_bucket)\n",
    "print(checkpoint_s3_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d27de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pt_estimator = PyTorch(\n",
    "    entry_point=\"bert_train_torchrun_trn1.py\", # Specify your train script\n",
    "    source_dir=\"train_scripts\",\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type='ml.trn1.2xlarge',\n",
    "    framework_version='1.11.0',\n",
    "    py_version='py38',\n",
    "    disable_profiler=True,\n",
    "    output_path=checkpoint_s3_bucket,\n",
    "    base_job_name=base_job_name,\n",
    "    \n",
    "    # Parameters required to enable checkpointing\n",
    "    checkpoint_s3_uri=checkpoint_s3_bucket,\n",
    "    volume_size = 512,\n",
    "    distribution={\n",
    "        \"torch_distributed\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "pt_estimator.fit({'train': training_input_path}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3762edb5",
   "metadata": {},
   "source": [
    "To find out S3 path where the trained weights are stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9aa3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_uri = pt_estimator.model_data \n",
    "print(model_path_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94e6133",
   "metadata": {},
   "source": [
    "Now that model training work is done, and the model weights are stored to S3, from here and on, we are going to focus on inference. The trained model weight file compressed and stored in the S3 path shown above. It is a dictionary, which contains keys that matche the original Hugging Face distilbert model. This means the trained weight may to run on any hardware platform, as long as the Hugging Face library installed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dfd2e3",
   "metadata": {},
   "source": [
    "# Inference using trained model\n",
    "\n",
    "In order to deploy the trained model using AWS Inferentia we need to do the following steps, This is needed inorder to optimize the model to get better performance on inferentia.\n",
    "\n",
    "1. Trace the trained model. The model needs to be converted to a torchscript format first.\n",
    "2.  Understanding the inference script.\n",
    "3. Compile and Deploy the model using Amazon SageMaker endpoints.\n",
    "4. Test the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ba983",
   "metadata": {},
   "source": [
    "## 1. Trace the trained model\n",
    "\n",
    "We have created a helper function which loads the trained model and runs trace on it. To trace a model into a torchscript (`.pth`), it requires the model and an `example_input`, which is a tensor or tuple of tensors. `torch.jit.trace` is actually recording all the tensor ops during the forward pass. The result is a torchscript file that captures the sequence of ops applied to the input tensor and all the way through the output nodes. The output is a model graph.\n",
    "\n",
    "this is a required step to have SageMaker Neo compile our model artifact, which will take a `tar.gz` file containing the traced model.\n",
    "\n",
    "Note: The `.pth` extension when saving our model is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a05ee7",
   "metadata": {},
   "source": [
    "Extract parts of `model_path_uri`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e8a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bucket = model_path_uri.replace(\"s3://\",\"\").split(\"/\")[0] # get bucket name.\n",
    "prefix_path = \"/\".join(model_path_uri.replace(\"s3://\",\"\").split(\"/\")[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc1fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.download_data(\"./\",model_bucket,prefix_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b68e1af",
   "metadata": {},
   "source": [
    "Now you should see a `model.tar.gz` in the current directory. This is the trained weights. Unzip it and put it in `distilbert` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd64119",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r bert\n",
    "! mkdir bert\n",
    "! tar -xvf ./model.tar.gz -C ./bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc21e3a",
   "metadata": {},
   "source": [
    "Now invoke the helper function with path to the model weights and sample input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f11e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from traceutils import trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32d7aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sample input for jit model tracing\n",
    "seq_0 = \"I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered controversial I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae1a103",
   "metadata": {},
   "source": [
    "Below, we will use `trace` function to convert this model into a torchscript via `torch.jit.trace` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6da268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_model_path = trace(seq_0,\"./bert/checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7977b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czvf traced_model.tar.gz -C traced_model model.pth && mv traced_model.tar.gz traced_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2337c96",
   "metadata": {},
   "source": [
    "We upload the traced model tar.gz file to Amazon S3, where our compilation job will download it from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec76f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new session for deployment in another region\n",
    "sess = sagemaker.Session(boto3.session.Session(region_name=\"us-east-2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82f2a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model_url = sess.upload_data(\n",
    "    path=\"traced_model/traced_model.tar.gz\",\n",
    "    key_prefix=\"neuron-experiments/bert-seq-classification/traced-model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927fbc79",
   "metadata": {},
   "source": [
    "## 2. Understanding the inference script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4990d6a8",
   "metadata": {},
   "source": [
    "Before we deploy any model, let's check out the code we have written to do inference on a SageMaker endpoint, with a default uncompiled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dba96af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pygmentize inference_scripts/inference_inf1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81acca5e",
   "metadata": {},
   "source": [
    "As usual, we have a `model_fn` - receives the model directory, is responsible for loading and returning the model -, an `input_fn` and `output_fn` - in charge of pre-processing/checking content types of input and output to the endpoint - and a `predict_fn`, which receives the outputs of `model_fn` and `input_fn` (meaning, the loaded model and the deserialized/pre-processed input data) and defines how the model will run inference.\n",
    "\n",
    "\n",
    "#### Now, lets see what changes in the inference code when we want to do inference with a model that has been compiled for Inferentia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd952136",
   "metadata": {},
   "source": [
    "```python\n",
    "# %load -s model_fn code/inference_inf1.py\n",
    "def model_fn(model_dir):\n",
    "    \n",
    "    model_dir = '/opt/ml/model/'\n",
    "    dir_contents = os.listdir(model_dir)\n",
    "    model_path = next(filter(lambda item: 'model' in item, dir_contents), None)\n",
    "    \n",
    "    tokenizer_init = AutoTokenizer.from_pretrained('bert-base-uncased', max_length=128)\n",
    "    model = torch.jit.load(os.path.join(model_dir, model_path))\n",
    "\n",
    "    \n",
    "    return (model, tokenizer_init)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02be6bc0",
   "metadata": {},
   "source": [
    "In this case, within the `model_fn` we first grab the model artifact located in `model_dir` (the compilation step will name the artifact `model_neuron.pt`, but we just get the first file containing `model` in its name for script flexibility). Then, **we load the Neuron compiled model with `torch.jit.load`**. \n",
    "\n",
    "Other than this change to `model_fn`, we only need to add an extra import `import torch_neuron` to the beginning of the script, and get rid of all `.to(device)` calls, since the Neuron runtime will take care of loading our model to the NeuronCores on our Inferentia instance. All other functions are unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24942ee5",
   "metadata": {},
   "source": [
    "## 3. Compile and Deploy the model using Amazon SageMaker endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db2aba",
   "metadata": {},
   "source": [
    "We now create a new `PyTorchModel` that will use `inference_inf1.py` as its entry point script. PyTorch version 1.9.0 is the latest that supports Neo compilation to Inferentia, as you can see from the warning in the compilation cell output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e70cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"neuron-experiments/bert-seq-classification\"\n",
    "flavour = \"normal\"\n",
    "date_string = datetime.now().strftime(\"%Y%m-%d%H-%M%S\")\n",
    "\n",
    "compiled_sm_model = PyTorchModel(\n",
    "    model_data=traced_model_url,\n",
    "    predictor_cls=Predictor,\n",
    "    framework_version=\"1.11.0\",\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    entry_point=\"inference_inf1.py\",\n",
    "    source_dir=\"inference_scripts\",\n",
    "    py_version=\"py38\",\n",
    "    name=f\"{flavour}-bert-pt181-{date_string}\",\n",
    "    env={\"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"10\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51ed04e",
   "metadata": {},
   "source": [
    "Finally, we are ready to compile the model and deploy it. Two notes here:\n",
    "* HuggingFace models should be compiled to `dtype` `int64`\n",
    "* the format for `compiler_options` differs from the standard Python `dict` that you can use when compiling for \"normal\" instance types; for inferentia, you must provide a JSON string with CLI arguments, which correspond to the ones supported by the [Neuron Compiler](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-cc/command-line-reference.html) (read more about `compiler_options` [here](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_OutputConfig.html#API_OutputConfig_Contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97cdc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hardware = \"inf1\"\n",
    "flavour = \"compiled-inf\"\n",
    "compilation_job_name = f\"bert-{flavour}-{hardware}-\" + date_string\n",
    "\n",
    "compile_start = time.time()\n",
    "compiled_inf1_model = compiled_sm_model.compile(\n",
    "    target_instance_family=f\"ml_{hardware}\",\n",
    "    input_shape={\"input_ids\": [1, 128], \"attention_mask\": [1, 128]},\n",
    "    job_name=compilation_job_name,\n",
    "    role=role,\n",
    "    framework=\"pytorch\",\n",
    "    framework_version=\"1.11.0\",\n",
    "    output_path=f\"s3://{sess.default_bucket()}/{prefix}/neo-compilations/{flavour}-model\",\n",
    "    compiler_options=json.dumps(\"--dtype int64\"),\n",
    "    #     compiler_options={'dtype': 'int64'},    # For compiling to \"normal\" instance types, cpu or gpu-based\n",
    "    compile_max_run=900,\n",
    ")\n",
    "compile_end = time.time()\n",
    "print('\\nSageMake Neo compile time: {} seconds'.format(compile_end - compile_start))\n",
    "\n",
    "# After successful compilation, we deploy our model to an inf1.xlarge instance.\n",
    "print(\"\\nCompilation successful !!! Deploying the model ...\")\n",
    "\n",
    "date_string = datetime.now().strftime(\"%Y%m-%d%H-%M%S\")\n",
    "\n",
    "deploy_start = time.time()\n",
    "\n",
    "compiled_inf1_predictor = compiled_inf1_model.deploy(\n",
    "    instance_type=\"ml.inf1.xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=f\"test-neo-{hardware}-{date_string}\",\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "deploy_end = time.time()\n",
    "print('\\nSageMaker endpoint deployment time: {} seconds'.format(deploy_end - deploy_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0178881",
   "metadata": {},
   "source": [
    "# 4. Test the model with Sample Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc9c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with model endpoint\n",
    "payload1 = \"The new Hugging Face SageMaker DLC makes it super easy to deploy models in production. I love it!\"\n",
    "compiled_inf1_predictor.predict(payload1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95edc481-21cc-4d4e-95e4-1f7094689d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with model endpoint\n",
    "payload1 = \"This movie is very bad\"\n",
    "compiled_inf1_predictor.predict(payload1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de4758d",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_inf1_predictor.delete_model()\n",
    "compiled_inf1_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7807499e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
